{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ilham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Dataset'\n",
    "trainSet = pd.read_csv(path + \"/Train Set.tsv\",header = None, delimiter=\"\\t\")\n",
    "testSet = pd.read_csv(path + \"/Test Set.tsv\",header = None, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimatTrain = []\n",
    "insertKalimat = ''\n",
    "for i in range(len(trainSet)):\n",
    "    insertKalimat += str(trainSet.loc[i,0]).lower()\n",
    "    insertKalimat += '/'\n",
    "    insertKalimat += str(trainSet.loc[i,1])\n",
    "    insertKalimat += '@'\n",
    "    if trainSet.loc[i,0] == \".\" and trainSet.loc[i,1] == \"Z\":\n",
    "        kalimatTrain.append(insertKalimat)\n",
    "        insertKalimat = ''\n",
    "kalimatTest = []\n",
    "insertKalimat = ''\n",
    "for i in range(len(testSet)):\n",
    "    insertKalimat += str(testSet.loc[i,0]).lower()\n",
    "    insertKalimat += '@'\n",
    "#     insertKalimat += str(testSet.loc[i,1])\n",
    "    if testSet.loc[i,0] == \".\" and testSet.loc[i,1] == \"Z\":\n",
    "        kalimatTest.append(insertKalimat)\n",
    "        insertKalimat = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(transition_prob, emission_prob, tokens):\n",
    "    # cek apakah semua token ada di vocab/emission probability\n",
    "    oov_status = 0\n",
    "    counter_check = 0\n",
    "    stop = False\n",
    "    while (counter_check < len(tokens)) and (not stop):\n",
    "        if tokens[counter_check] not in vocabs.values():\n",
    "            stop = True\n",
    "            oov_status = 1\n",
    "        else:\n",
    "            counter_check += 1\n",
    "    \n",
    "    if oov_status == 1:\n",
    "        # kalimat uji mengandung unknown word/OOV, tanpa smoothing tidak bisa diproses\n",
    "        print('kalimat uji mengandung unknown word')\n",
    "        return None, None\n",
    "    else:\n",
    "        # create a path probability matrix viterbi[N,T]\n",
    "        # N: banyaknya state\n",
    "        # T: jumlah token\n",
    "\n",
    "        T, N = len(tokens)+1, len(tags) # token ditambah satu untuk start\n",
    "        new_tokens = ['<s>'] + tokens\n",
    "        print('T=',T,',N=',N)\n",
    "        print('token:',new_tokens)\n",
    "        viterbi_mat = [[0 for x in range(T)] for y in range(N)] \n",
    "        # create backpointers matrix\n",
    "        backpointers = [[0 for x in range(T)] for y in range(N)] \n",
    "\n",
    "        # initial probability distribution over states (phi)\n",
    "        # transition probability dengan previous state adalah <start>\n",
    "        phi = {}\n",
    "        for i in range (1,len(tags)):\n",
    "            phi[tags[i]] = transition_prob[('<start>',tags[i])]\n",
    "                    \n",
    "        # initialization\n",
    "        # urutan index state sesuai dengan index di dictionary tags{}\n",
    "        # inisialisasi dimulai dari state ke-1, state ke-0 sudah pasti <start>\n",
    "        viterbi_mat[0][0] = 1.0 # untuk token <s>, tag = <start>\n",
    "        for s in range(1,N):\n",
    "            viterbi_mat[s][1] = phi[tags[s]] * emission_prob[(tags[s],new_tokens[1])]\n",
    "            backpointers[s][1] = 0\n",
    "\n",
    "\n",
    "        # recursion step\n",
    "        for t in range(2,T):\n",
    "            for s in range(1,N):\n",
    "                # get max viterbi from previous transition\n",
    "                max_prev_transition = 0.0\n",
    "                max_state = 0\n",
    "                for i in range(1,N):                \n",
    "                    #selain transisi dari tag <start>, range mulai dari indeks 1\n",
    "                    temp_transition = viterbi_mat[i][t-1] * transition_prob[(tags[i],tags[s])]\n",
    "                    if temp_transition > max_prev_transition:\n",
    "                        max_prev_transition = temp_transition\n",
    "                        max_state = i\n",
    "                viterbi_mat[s][t] = max_prev_transition * emission_prob[(tags[s],new_tokens[t])]\n",
    "                backpointers[s][t] = max_state\n",
    "\n",
    "        # terminasi\n",
    "        # get max probability in last column\n",
    "        max_last_prob = 0.0\n",
    "        best_last_tag = ''\n",
    "        idx_best_last_tag = 0\n",
    "        for i in range (1,N):\n",
    "            if viterbi_mat[i][T-1] > max_last_prob:\n",
    "                max_last_prob = viterbi_mat[i][T-1]\n",
    "                best_last_tag = tags[i]\n",
    "                idx_best_last_tag = i\n",
    "\n",
    "        best_path = []\n",
    "        best_path.append(idx_best_last_tag)\n",
    "        for i in range(T-1,1,-1):\n",
    "            best_prev_tag = backpointers[idx_best_last_tag][i]\n",
    "            best_path.append(best_prev_tag)\n",
    "        # reverse the order\n",
    "        best_path = best_path[::-1]\n",
    "    \n",
    "        return viterbi_mat, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = {}\n",
    "for i in range(len(trainSet)):\n",
    "    vocabs[i] = str(trainSet.iloc[i,0]).lower()\n",
    "# tags = {}\n",
    "# for i in range(len(trainSet)):\n",
    "#     tags[i] = str(trainSet.iloc[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "counter = 0\n",
    "for i in range(len(trainSet)):\n",
    "    if trainSet.loc[i,0] == \".\" and trainSet.loc[i,1] == \"Z\":\n",
    "        tags[counter] = str(trainSet.iloc[i,1])\n",
    "        counter += 1\n",
    "        tags[counter] = '<start>'\n",
    "    else:\n",
    "        tags[counter] = str(trainSet.iloc[i,1])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilityWord = []\n",
    "for i in range (len(trainSet)):\n",
    "    possibilityWord.append(trainSet.loc[i,0].lower())\n",
    "possibilityWord = set(possibilityWord) \n",
    "possibilityWord = (list(possibilityWord)) \n",
    "len(possibilityWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilityTag = []\n",
    "for i in range (len(tags)):\n",
    "    possibilityTag.append(tags[i])\n",
    "possibilityTag = set(possibilityTag) \n",
    "possibilityTag = (list(possibilityTag))\n",
    "len(possibilityTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14322"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissionProb = {}\n",
    "# possibilityTagEP = copy.deepcopy(possibilityTag)\n",
    "# possibilityTagEP.remove('<start>')\n",
    "for tag in possibilityTag:\n",
    "    for word in possibilityWord:\n",
    "        emissionProb[tag,word] = 0\n",
    "for i in range (len(trainSet)):\n",
    "    check = str(trainSet.loc[i,1]), str(trainSet.loc[i,0]).lower()\n",
    "    if check in emissionProb:\n",
    "        emissionProb[str(trainSet.loc[i,1]), str(trainSet.loc[i,0]).lower()] += 1  \n",
    "import copy\n",
    "emissionProbClc = copy.deepcopy(emissionProb)\n",
    "for tag, word in emissionProb:\n",
    "    total = 0\n",
    "    for wordd in possibilityWord:\n",
    "        total += emissionProbClc[tag, wordd]\n",
    "    if total == 0:\n",
    "        emissionProb[tag, word] = 0\n",
    "    else:\n",
    "        emissionProb[tag, word] = emissionProb[tag, word] / total\n",
    "len(emissionProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitionProb = {}\n",
    "for tag1 in possibilityTag:\n",
    "    for tag2 in possibilityTag:\n",
    "        transitionProb[tag1,tag2] = 0\n",
    "# transitionProb\n",
    "for i in range (1, len(tags)):\n",
    "    tagXtag = tags[i-1], tags[i]\n",
    "    if tagXtag in transitionProb:\n",
    "        transitionProb[tagXtag] += 1\n",
    "import copy\n",
    "transitionProbClc = copy.deepcopy(transitionProb)\n",
    "for tag1, tag2 in transitionProb:\n",
    "    total = 0\n",
    "    for tag in possibilityTag:\n",
    "        total += transitionProbClc[tag1, tag]\n",
    "    transitionProb[tag1, tag2] = transitionProb[tag1, tag2] / total\n",
    "len(transitionProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T= 8 ,N= 1383\n",
      "token: ['<s>', 'menteri', 'pertahanan', 'as', 'dijadwalkan', 'mengunjungi', 'india', '.']\n",
      "T= 9 ,N= 1383\n",
      "token: ['<s>', 'tata', 'power', 'menyuplai', 'batu bara', 'pada', 'tahun', '2000', '.']\n",
      "T= 7 ,N= 1383\n",
      "token: ['<s>', 'pemerintah', 'hati-hati', 'dalam', 'mengelola', 'bumn', '.']\n",
      "T= 9 ,N= 1383\n",
      "token: ['<s>', 'perusahaan', 'baru', 'tersebut', 'mencanangkan', 'target', 'perolehan', 'laba bersih', '.']\n",
      "T= 7 ,N= 1383\n",
      "token: ['<s>', 'menteri', 'pertahanan', 'mengunjungi', 'pangkalan', 'udara', '.']\n",
      "T= 9 ,N= 1383\n",
      "token: ['<s>', 'menurut', 'laporan', 'sekretaris', 'perusahaan', ',', 'laba bersih', 'meningkat', '.']\n",
      "T= 8 ,N= 1383\n",
      "token: ['<s>', 'transaksi', 'penjualan', 'barang mewah', 'tahun', '2007', 'turun', '.']\n",
      "T= 10 ,N= 1383\n",
      "token: ['<s>', 'menkeu', 'memperkirakan', 'inflasi', 'akan', 'meningkat', 'dibanding', 'tahun', 'lalu', '.']\n",
      "T= 10 ,N= 1383\n",
      "token: ['<s>', 'kenaikan', 'tarif', 'didorong', 'oleh', 'target', 'laba bersih', 'yang', 'meningkat', '.']\n",
      "T= 10 ,N= 1383\n",
      "token: ['<s>', 'makanan', 'dari', 'luar negeri', 'tidak', 'bisa', 'masuk', 'pasar', 'lokal', '.']\n"
     ]
    }
   ],
   "source": [
    "predictedHMM = []\n",
    "for i in range (len(kalimatTest)):\n",
    "    checkThis = kalimatTest[i].split('@')\n",
    "    checkThis.pop()\n",
    "    viterbi_mat, best_path =  viterbi(transitionProb, emissionProb, checkThis)\n",
    "    for tag in best_path:\n",
    "        predictedHMM.append(tags[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "dfPredictedHMM = copy.deepcopy(testSet)\n",
    "dfPredictedHMM = dfPredictedHMM.rename(columns={0: \"Kata\", 1: \"Tag\"})\n",
    "dfPredictedHMM.insert(2, \"Predicted Tag\", predictedHMM)\n",
    "dfPredictedHMM.to_excel(\"Hasil HMM.xlsx\", index =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi:  0.8961038961038961\n"
     ]
    }
   ],
   "source": [
    "accuracyHMM = 0\n",
    "for i in range (len(dfPredictedHMM)):\n",
    "    if dfPredictedHMM.iloc[i,1] == dfPredictedHMM.iloc[i,2]:\n",
    "        accuracyHMM += 1\n",
    "accuracyHMM = accuracyHMM / len(dfPredictedHMM)\n",
    "print(\"Akurasi: \", accuracyHMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
